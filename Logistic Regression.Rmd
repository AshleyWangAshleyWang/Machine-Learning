## Logistic Regression

We will use the [Cleveland clinic heart disease dataset](https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset). The goal is to predict the label `num > 0`. 

```{r}
heart = read.csv("processed_cleveland.csv")
heart$Y = as.factor(heart$num > 0)
heart = subset(heart, select = -c(num, ca, thal))
```

We are going to perform three models: 

  * A logistic regression 
  * A logistic regression with Ridge penalty

And we will evaluate them using two different criteria:

  * Classification error
  * Area under the ROC curve

Also, to keep things simpler, we will not use cross-validation for this question. Instead, all the evaluations will be just on the training dataset. We are of course at the risk of over-fitting, but Part III will address that issue. In addition, since no cross-validation is needed, you should be using the `glmnet()` function instead of `cv.glmnet()`. The syntax of this function is almost identical to its cross-validation version, except that you will not have the cross-validation feature to help you select the best $\lambda$. However, the function will still produce all the coefficients for each $\lambda$ value. If you need more details, please see the documentation provided at [CRAN](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf). 

### Part I

Complete the following questions for logistic regression:

* Fit logistic regression to the heart data and report the most significant variable. 
  
```{r} 
# fit logistic regression
logistic.fit <- glm(Y~., data = heart, family = binomial)
# find the most sig variable by p value
which.min(summary(logistic.fit)$coefficients[,"Pr(>|z|)"])
```

The most significant variable is 'sex'.
  
* Using 0.5 as the cut-off value of predicted probability, produce the confusion table of the training data. What is the classification error associated with this model?
  
```{r}
# use 0.5 as the cut-off value
pred = predict(logistic.fit, newdata = heart, type = "response")
yhat = (logistic.fit$fitted.values>0.5)
# create confusion table
table(yhat, heart$Y)
``` 
```{r}
# classification error
(26+34)/nrow(heart)
```
The classification error of this model is 0.1980.  

* What is the sensitivity and specificity of this model? Choose a new cut-off value that would give a higher sensitivity, and report the confusion table and sensitivity associated with this new cut-off value. 
  
```{r}
# sensitivity
sensitivity = 105/(105+34)
sensitivity
# specificity
specificity =  138/(138+26)
specificity
```

The sensitivity of this model with 0.5 as cut off value is 0.7554 and the specificity of this model is 0.8415.


### Choose a new cut-off value, I select 0.2 as my new cut-off value:
```{r}
# choose new cut-off
yhat = (logistic.fit$fitted.values>0.2)
table(yhat, heart$Y)
```

```{r}
# sensitivity
n_sensitivity = 132/(132+7)
n_sensitivity
# specificity
n_specificity =  90/(90+74)
n_specificity
```
The new sensitivity of this model with 0.2 as cut off value is 0.9496 and the specificity of this model is 0.5488.

* Produce the ROC curve plot associated with your logistic regression and report the AUC. 

```{r, results='hide', message=FALSE, warning=FALSE}
library(ROCR)
```

```{r}
roc <- prediction(pred, heart$Y)
  
# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)
```
```{r}
# report auc
performance(roc, measure = "auc")@y.values[[1]]
```
The AUC is 0.8894.

### Part II

Complete the following questions for logistic regression with Ridge penalty :

* Use the `glmnet()` function to produce a set of coefficients across many $\lambda$ values. 
  
```{r, results='hide', message=FALSE, warning=FALSE}
library(glmnet)
``` 
```{r}
lasso.fit = glmnet(x = data.matrix(heart[, 1:11]), 
                   y = heart[,12], nlambda = 100, alpha=0, 
                   family = "binomial")
```  

* Since we will not perform cross-validation, let's just use one of the $\lambda$ values. You can extract all the coefficients using the `coef()` function. This will give you a matrix of 100 columns, associated with 100 different $\lambda$ values. Let's use the coefficients associated with the 40th smallest $\lambda$ value. Based on these coefficients, calculate the predicted (using training data) probabilities of all observations. Use a histogram to plot all of them. 

```{r}
# select the 40 th smallest lambda
lambda40 = sort(lasso.fit$lambda)[40]
```
```{r}
coef(lasso.fit, s = lambda40)
```
```{r}
# predict with 40th smallest lambda
prediction2 <- predict(lasso.fit, newx = data.matrix(heart[,1:11]), s=lambda40, type = "response")

# draw a histogram
hist(prediction2)
```

* Using 0.5 as the cut-off value of predicted probability, produce the confusion table of the training data. What is the classification error associated with this model?

```{r}
# Use 0.5 as the cut-off value of predicted probability
yhat = (prediction2>0.5)
table(yhat, heart$Y)
```
```{r}
# classification error 
(43+15)/nrow(heart)
```
The classification error of this model is 0.1914.

* Produce the ROC curve plot associated with your model and report the AUC. 

```{r}
roc <- prediction(prediction2, heart$Y)
  
# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)
```
```{r}
# report auc
performance(roc, measure = "auc")@y.values[[1]]
```
The AUC is 0.8759.


### Part III

In this last part, we will use a built-in feature of the `glmnet` package. Read the documentation of the `cv.glmnet()` function at [CRAN](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) and understand how to specify the `type.measure` argument so that the cross-validation uses the AUC as the selection criterion of $\lambda$ to pick the best model. Implement a 10-fold cross-validation Ridge regression using our data and report the best $\lambda$ value (`"lambda.min"`). What is the cross-validation AUC associated with this penalty?

```{r}
set.seed(670144148)
lasso.fit = cv.glmnet(x = data.matrix(heart[, 1:11]), 
                      y = heart[,12], nfold = 10, 
                      type.measure="auc", 
                      alpha = 0, family = "binomial")
```
```{r}
# get the best lambda value
lasso.fit$lambda.min
```
```{r}
plot(lasso.fit)
```
```{r}
max(lasso.fit$cvm)
```
The best lambda value for my model is 0.558477, and the cross-validation AUC is 0.8729.
